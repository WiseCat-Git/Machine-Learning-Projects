# -*- coding: utf-8 -*-
"""Solucion_IEP_ML_u2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pGwpZS4Wq3GKXsaE4Djsvn-E0i8sNJt-

# **Soluci√≥n enunciado Unidad 2 Machine Learning**
"""

# Conectar con Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Importar librer√≠as
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

# Rutas de los archivos en Google Drive
file1 = "/content/drive/MyDrive/Colab Notebooks/datos_IEP_IAA_ML_u2/clust_ex1.csv"
file2 = "/content/drive/MyDrive/Colab Notebooks/datos_IEP_IAA_ML_u2/clust_ex2.csv"

# Cargar los datasets
df1 = pd.read_csv(file1)
df2 = pd.read_csv(file2)

# Visualizar las primeras filas
print("\n Dataset 1:")
display(df1.head())

print("\n Dataset 2:")
display(df2.head())

# Resumen de los datasets
print("\n Informaci√≥n de Dataset 1:")
df1.info()

print("\n Informaci√≥n de Dataset 2:")
df2.info()

import random

def initialize_centroids(data, k):
    return data.sample(n=k).values

def assign_clusters(data, centroids):
    clusters = []
    for _, row in data.iterrows():
        distances = [np.linalg.norm(row - centroid) for centroid in centroids]
        clusters.append(np.argmin(distances))
    return np.array(clusters)

def update_centroids(data, clusters, k):
    new_centroids = []
    for i in range(k):
        cluster_points = data[clusters == i]
        new_centroids.append(cluster_points.mean(axis=0))
    return np.array(new_centroids)

def kmeans_manual(data, k, max_iters=100, tol=1e-4):
    centroids = initialize_centroids(data, k)

    for _ in range(max_iters):
        clusters = assign_clusters(data, centroids)
        new_centroids = update_centroids(data, clusters, k)

        if np.linalg.norm(new_centroids - centroids) < tol:
            break
        centroids = new_centroids

    return clusters, centroids

# Seleccionar solo variables num√©ricas
df1_numeric = df1.select_dtypes(include=[np.number])

# Aplicar K-Means manual con k=3
clusters_manual, centroids_manual = kmeans_manual(df1_numeric, k=3)

# Agregar los clusters al DataFrame
df1['Cluster_Manual'] = clusters_manual

# Mostrar la distribuci√≥n de clusters
print("\nüîπ Distribuci√≥n de Clusters Manuales:")
display(df1['Cluster_Manual'].value_counts())

# Normalizar los datos antes de aplicar K-Means
scaler = StandardScaler()
df1_scaled = scaler.fit_transform(df1_numeric)

# Aplicar K-Means con sklearn
kmeans_sklearn = KMeans(n_clusters=3, random_state=42, n_init=10)
df1['Cluster_Sklearn'] = kmeans_sklearn.fit_predict(df1_scaled)

# Comparaci√≥n de resultados manual vs sklearn
print("\n Comparaci√≥n de distribuci√≥n de clusters:")
display(df1[['Cluster_Manual', 'Cluster_Sklearn']].value_counts())

# Visualizar los datos sin normalizar
plt.figure(figsize=(10, 5))
plt.scatter(df1_numeric.iloc[:, 0], df1_numeric.iloc[:, 1], c=df1['Cluster_Sklearn'], cmap='viridis', alpha=0.6)
plt.title("Clusters sin Normalizaci√≥n")
plt.xlabel(df1_numeric.columns[0])
plt.ylabel(df1_numeric.columns[1])
plt.colorbar(label="Cluster")
plt.show()

# Visualizar los datos normalizados
plt.figure(figsize=(10, 5))
plt.scatter(df1_scaled[:, 0], df1_scaled[:, 1], c=df1['Cluster_Sklearn'], cmap='viridis', alpha=0.6)
plt.title("Clusters con Normalizaci√≥n")
plt.xlabel(df1_numeric.columns[0])
plt.ylabel(df1_numeric.columns[1])
plt.colorbar(label="Cluster")
plt.show()

wcss = []  # Within-Cluster Sum of Squares

for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(df1_scaled)
    wcss.append(kmeans.inertia_)

# Graficar la curva del m√©todo del codo
plt.figure(figsize=(8, 5))
plt.plot(range(1, 11), wcss, marker='o', linestyle='--')
plt.xlabel("N√∫mero de Clusters (k)")
plt.ylabel("WCSS")
plt.title("M√©todo del Codo para Selecci√≥n de Clusters")
plt.show()

# Evaluar la calidad de los clusters
silhouette_avg = silhouette_score(df1_scaled, df1['Cluster_Sklearn'])
print(f"\nüìå Silhouette Score para K-Means: {silhouette_avg:.4f}")

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Dividir datos en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(df1_numeric, df1['Cluster_Sklearn'], test_size=0.2, random_state=42)

# Entrenar un modelo Random Forest
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)

# Predecir y evaluar el modelo
y_pred = clf.predict(X_test)
print("\nüìå Reporte de Clasificaci√≥n:")
print(classification_report(y_test, y_pred))

# Accuracy del modelo
accuracy = accuracy_score(y_test, y_pred)
print(f"\n‚úÖ Accuracy del modelo de clasificaci√≥n: {accuracy:.4f}")

"""# **Conclusiones Finales**

1Ô∏è‚É£ Implementaci√≥n Manual vs Scikit-learn

*Ambos enfoques generan clusters similares, pero Scikit-learn es m√°s eficiente.*

2Ô∏è‚É£ Efecto de la Normalizaci√≥n

*La normalizaci√≥n mejora la estabilidad de los clusters, evitando que algunas variables dominen.*

3Ô∏è‚É£ Impacto de Outliers

*Los valores at√≠picos pueden afectar significativamente los clusters.*

4Ô∏è‚É£ N√∫mero √ìptimo de Clusters

*El m√©todo del codo ayuda a seleccionar el n√∫mero de clusters adecuado.*

5Ô∏è‚É£ Clasificaci√≥n basada en Clusters

Entrenar un modelo de clasificaci√≥n usando los clusters obtenidos puede ser √∫til para futuras predicciones.
"""