# -*- coding: utf-8 -*-
"""Solucion_enunciados_I&II_IEP-IAA-ML-u3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OMmPONYyxj2sjPCOQDotz_-0DxdafVdn

# **Solucion enunciado I**

Vamos a resolver el caso practico de aprendizaje semi-supervisado paso a paso en Google Colab utilizando Scikit-learn explorando los tres modelos mencionados, pero tambiÃ©n, realizarÃ© un repaso sobre otros posibles enfoques y Ã¡ngulos de soluciÃ³n.

El siguiente cÃ³digo realizarÃ¡:

1. La carga de los datos desde Google Drive
2. Una exploraciÃ³n de la estructura del dataset para ver quÃ© informaciÃ³n contiene.
3. Una verificaciÃ³n de cuÃ¡ntos datos estÃ¡n etiquetados en el set de entrenamiento.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.semi_supervised import LabelPropagation, SelfTrainingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from google.colab import drive

# Montar Google Drive para acceder a los archivos
drive.mount('/content/drive')

# Cargar los datasets
train_path = "/content/drive/MyDrive/Colab Notebooks/datos_IEP_ML_u3/semisupervisado_train.csv"
test_path = "/content/drive/MyDrive/Colab Notebooks/datos_IEP_ML_u3/semisupervisado_test.csv"

df_train = pd.read_csv(train_path)
df_test = pd.read_csv(test_path)

# Mostrar informaciÃ³n general de los datos
print("\nğŸ“Œ InformaciÃ³n del dataset de entrenamiento")
df_train.info()
print("\nğŸ“Œ InformaciÃ³n del dataset de prueba")
df_test.info()

# Mostrar ejemplos de los datos
print("\nğŸ” Primeras filas del dataset de entrenamiento")
print(df_train.head())
print("\nğŸ” Primeras filas del dataset de prueba")
print(df_test.head())

# Revisar cantidad de datos etiquetados y no etiquetados
print("\nâœ… Cantidad de datos etiquetados en entrenamiento:", df_train['target'].notna().sum())
print("âŒ Cantidad de datos NO etiquetados en entrenamiento:", df_train['target'].isna().sum())
print("ğŸ¯ Cantidad de datos en test:", df_test.shape[0])

"""# **Paso 1: Modelo de RegresiÃ³n LogÃ­stica con Solo 6 Muestras Etiquetadas**

Dado que solo tenemos 6 muestras etiquetadas, entrenaremos un modelo de RegresiÃ³n LogÃ­stica para ver quÃ© tan bien generaliza con tan pocos datos.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

# Ruta de los archivos en Google Drive
train_path = "/content/drive/MyDrive/Colab Notebooks/datos_IEP_ML_u3/semisupervisado_train.csv"
test_path = "/content/drive/MyDrive/Colab Notebooks/datos_IEP_ML_u3/semisupervisado_test.csv"

# Cargar los datos
train_data = pd.read_csv(train_path)
test_data = pd.read_csv(test_path)

# Verificar que los datos se cargaron correctamente
print("\nâœ… Dataset de entrenamiento cargado:")
print(train_data.head())

print("\nâœ… Dataset de prueba cargado:")
print(test_data.head())

# Verificar si la columna target tiene valores nulos
print("\nğŸ” Cantidad de valores nulos en target de train:", train_data["target"].isnull().sum())

# Separar datos etiquetados de los no etiquetados en el conjunto de entrenamiento
labeled_data = train_data.dropna()  # Solo 6 muestras etiquetadas
X_train_labeled = labeled_data.drop(columns=["target"])
y_train_labeled = labeled_data["target"]

# Definir X y y del conjunto de prueba
X_test = test_data.drop(columns=["target"])
y_test = test_data["target"]

# Entrenar el modelo de RegresiÃ³n LogÃ­stica
logistic_model = LogisticRegression()
logistic_model.fit(X_train_labeled, y_train_labeled)

# Hacer predicciones
y_pred = logistic_model.predict(X_test)

# EvaluaciÃ³n del modelo
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("\nğŸ“Œ **Resultados del Modelo de RegresiÃ³n LogÃ­stica con 6 muestras etiquetadas**")
print(f"ğŸ¯ Accuracy: {accuracy:.4f}")
print(f"ğŸ¯ Precision: {precision:.4f}")
print(f"ğŸ¯ Recall: {recall:.4f}")
print(f"ğŸ¯ F1-Score: {f1:.4f}")
print("\nğŸ” **Reporte de ClasificaciÃ³n:**\n", classification_report(y_test, y_pred))

"""# **Paso 2: Modelo de PropagaciÃ³n de Etiquetas**

Ahora aplicamos Label Propagation, que utiliza todos los datos (etiquetados y no etiquetados) para mejorar las predicciones.
"""

from sklearn.semi_supervised import LabelPropagation

# Crear copia del dataset para etiquetar correctamente
X_train = train_data.drop(columns=["target"]).copy()
y_train = train_data["target"].copy()

# Asignar -1 a las muestras no etiquetadas
y_train.fillna(-1, inplace=True)

# Entrenar modelo de propagaciÃ³n de etiquetas
label_propagation_model = LabelPropagation()
label_propagation_model.fit(X_train, y_train)

# Hacer predicciones
y_pred = label_propagation_model.predict(X_test)

# EvaluaciÃ³n del modelo
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("\nğŸ“Œ **Resultados del Modelo de PropagaciÃ³n de Etiquetas**")
print(f"ğŸ¯ Accuracy: {accuracy:.4f}")
print(f"ğŸ¯ Precision: {precision:.4f}")
print(f"ğŸ¯ Recall: {recall:.4f}")
print(f"ğŸ¯ F1-Score: {f1:.4f}")
print("\nğŸ” **Reporte de ClasificaciÃ³n:**\n", classification_report(y_test, y_pred))

"""# **Paso 3: Modelo de Self-Learning**

Ahora aplicamos Self-Training Classifier, que utiliza un modelo base (RegresiÃ³n LogÃ­stica o SVM) y entrena iterativamente con datos no etiquetados.
"""

from sklearn.semi_supervised import SelfTrainingClassifier

# Modelo base: RegresiÃ³n LogÃ­stica
base_model = LogisticRegression()

# Crear Self-Training Classifier
self_training_model = SelfTrainingClassifier(base_model)
self_training_model.fit(X_train, y_train)

# Hacer predicciones
y_pred = self_training_model.predict(X_test)

# EvaluaciÃ³n del modelo
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("\nğŸ“Œ **Resultados del Modelo de Self-Learning con RegresiÃ³n LogÃ­stica**")
print(f"ğŸ¯ Accuracy: {accuracy:.4f}")
print(f"ğŸ¯ Precision: {precision:.4f}")
print(f"ğŸ¯ Recall: {recall:.4f}")
print(f"ğŸ¯ F1-Score: {f1:.4f}")
print("\nğŸ” **Reporte de ClasificaciÃ³n:**\n", classification_report(y_test, y_pred))

"""# **ğŸ”‘ AnÃ¡lisis de Resultados**

1. RegresiÃ³n LogÃ­stica con solo 6 muestras etiquetadas:

  Como era de esperarse, el modelo tiene un desempeÃ±o bajo, con accuracy del 47%.
  No es suficiente informaciÃ³n para que un modelo supervisado generalice bien.
  Problema: La cantidad de datos etiquetados es extremadamente pequeÃ±a.

2. PropagaciÃ³n de Etiquetas (Label Propagation):

  Mejor mejora en recall (92.3%): Significa que el modelo es muy bueno en identificar correctamente la clase positiva (1).
  Sin embargo, tiene un problema de precisiÃ³n baja (57.1%), lo que sugiere que el modelo tambiÃ©n genera muchos falsos positivos.
  ConclusiÃ³n: Es un mÃ©todo efectivo para mejorar el desempeÃ±o inicial, pero puede generar ruido si los datos no estÃ¡n bien distribuidos.

3. Self-Learning con RegresiÃ³n LogÃ­stica:

  Logra una mejora moderada en accuracy (50%) respecto al modelo de RegresiÃ³n LogÃ­stica puro.
  F1-score (54.6%) mejora ligeramente con respecto a la RegresiÃ³n LogÃ­stica inicial.
  ConclusiÃ³n: Self-Learning puede ser Ãºtil, pero en este caso no aporta tanta mejora como Label Propagation.

# **Mejor Modelo: PropagaciÃ³n de Etiquetas (Label Propagation)**

Accuracy mÃ¡s alto (60%)

Recall muy alto (92.3%), lo que es ideal si el objetivo es detectar la mayor cantidad de casos positivos.

Sin embargo, hay un problema de precisiÃ³n, lo que sugiere que el modelo puede estar sobreajustando algunas predicciones.

Para optimizar el rendimiento, podemos probar:

1. Usar un modelo base mÃ¡s complejo en Self-Learning â†’ Por ejemplo, Support Vector Machines (SVM).

2. Ajustar hiperparÃ¡metros en Label Propagation â†’ Especialmente el nÃºmero de iteraciones y el kernel.

3. Aumentar la cantidad de datos etiquetados â†’ PodrÃ­amos usar estrategias de Active Learning para elegir quÃ© muestras etiquetar.

4. CombinaciÃ³n de ambos mÃ©todos â†’ Primero usar Label Propagation y luego Self-Learning.

Vamos a elegir probar con Self-learning con SVM
"""

# Importar librerÃ­as necesarias
import numpy as np
import pandas as pd
from sklearn.svm import SVC
from sklearn.semi_supervised import SelfTrainingClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import StandardScaler

# Cargar los datos
train_path = "/content/drive/MyDrive/Colab Notebooks/datos_IEP_ML_u3/semisupervisado_train.csv"
test_path = "/content/drive/MyDrive/Colab Notebooks/datos_IEP_ML_u3/semisupervisado_test.csv"

train_data = pd.read_csv(train_path)
test_data = pd.read_csv(test_path)

# Separar caracterÃ­sticas y etiquetas en el dataset de prueba
X_test = test_data.drop(columns=["target"])
y_test = test_data["target"]

# Separar datos etiquetados y no etiquetados en el dataset de entrenamiento
labeled_data = train_data.dropna()  # Solo las 6 muestras etiquetadas
unlabeled_data = train_data[train_data["target"].isna()]  # 990 muestras sin etiqueta

X_train_labeled = labeled_data.drop(columns=["target"])
y_train_labeled = labeled_data["target"]

X_train_unlabeled = unlabeled_data.drop(columns=["target"])
y_train_unlabeled = np.full(len(X_train_unlabeled), -1)  # Etiquetas -1 para datos sin etiqueta

# Unir datos etiquetados y no etiquetados
X_train = pd.concat([X_train_labeled, X_train_unlabeled])
y_train = np.concatenate([y_train_labeled, y_train_unlabeled])

# Normalizar los datos
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Crear modelo SVM para Self-Learning
base_svm = SVC(probability=True, kernel='rbf', gamma='scale')  # Probabilidad activada para Self-Learning

# Crear Self-Learning con SVM
self_learning_svm = SelfTrainingClassifier(base_svm, max_iter=1000, verbose=True)

# Entrenar modelo
self_learning_svm.fit(X_train, y_train)

# Predicciones en el dataset de prueba
y_pred = self_learning_svm.predict(X_test)

# EvaluaciÃ³n del modelo
accuracy = accuracy_score(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)

# Resultados
print("\nğŸ“Œ **Resultados del Modelo de Self-Learning con SVM**")
print(f"ğŸ¯ Accuracy: {accuracy:.4f}")
print("\nğŸ” **Reporte de ClasificaciÃ³n:**")
print(classification_rep)

"""**Observaciones clave:**

El modelo de Self-Learning con SVM tiene una accuracy ligeramente superior (0.53) que RegresiÃ³n LogÃ­stica (0.50), pero inferior a PropagaciÃ³n de Etiquetas (0.60).

Mejor recall para la clase 1 (0.71 vs. 0.57 en RegresiÃ³n LogÃ­stica).
Indica que el modelo es mejor capturando la clase 1.

Peor precisiÃ³n en la clase 0 (0.52 vs. 0.57 en PropagaciÃ³n de Etiquetas).

Puede indicar un desbalance en la clasificaciÃ³n de la clase negativa.

El modelo de PropagaciÃ³n de Etiquetas sigue siendo el mÃ¡s efectivo en tÃ©rminos de accuracy y recall.

Vamos a proceder a ajustar hiperparÃ¡meetros en SVM, cambiamos kernel a linear, poly, sigmoid en lugar de rbf, ajustamos C donde se controla la penalizacion de errores y modificamos gamma que controla la influencia de cada punto.

Aumentamos las iteraciones de self-learning, propagamos etiquetas combinadas con self-learning.
"""

import pandas as pd
import numpy as np
from sklearn.semi_supervised import SelfTrainingClassifier, LabelPropagation
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score
from sklearn.model_selection import GridSearchCV
from google.colab import drive

# Montar Google Drive
drive.mount('/content/drive')

# Cargar datasets
train_path = "/content/drive/MyDrive/Colab Notebooks/datos_IEP_ML_u3/semisupervisado_train.csv"
test_path = "/content/drive/MyDrive/Colab Notebooks/datos_IEP_ML_u3/semisupervisado_test.csv"

train_data = pd.read_csv(train_path)
test_data = pd.read_csv(test_path)

# Separar caracterÃ­sticas y etiquetas
X_train = train_data.drop(columns=["target"])
y_train = train_data["target"]
X_test = test_data.drop(columns=["target"])
y_test = test_data["target"]

# Manejo de valores no etiquetados
y_train_unlabeled = y_train.copy()
y_train_unlabeled[np.isnan(y_train_unlabeled)] = -1  # Configurar valores no etiquetados

# Aplicamos propagaciÃ³n de etiquetas antes del self-learning
label_prop_model = LabelPropagation(max_iter=1000)
label_prop_model.fit(X_train, y_train_unlabeled)
y_train_propagated = label_prop_model.transduction_  # Recuperamos etiquetas propagadas

# Definir hiperparÃ¡metros a ajustar en SVM
param_grid = {
    'kernel': ['linear', 'poly', 'sigmoid'],  # Cambiamos el kernel
    'C': [0.1, 1, 10],  # Ajustamos la penalizaciÃ³n de errores
    'gamma': ['scale', 'auto']  # Modificamos la influencia de cada punto
}

# Hacemos la optimizaciÃ³n de hiperparÃ¡metros en SVM
svm = SVC()
grid_search = GridSearchCV(svm, param_grid, cv=3)
grid_search.fit(X_train, y_train_propagated)
best_svm = grid_search.best_estimator_  # Mejor modelo SVM encontrado

# Aplicamos Self-Learning con el mejor SVM
self_learning_model = SelfTrainingClassifier(best_svm, max_iter=50)  # Aumentamos iteraciones
self_learning_model.fit(X_train, y_train_propagated)

# Evaluamos en test
y_pred = self_learning_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

# Mostrar resultados
accuracy, report

"""Los resultados muestran una mejora en recall para la clase positiva (1.0) y una precisiÃ³n mÃ¡s equilibrada en comparaciÃ³n con los modelos anteriores.

Sin embargo, la precisiÃ³n de la clase 0.0 sigue siendo baja, lo que indica que el modelo tiene dificultades para identificar correctamente ejemplos negativos.

PodrÃ­amos hacer una selecciÃ³n de las muestras con mayor confianza antes de propagarlas.
"""

# ğŸ”¹ Importar LibrerÃ­as
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.svm import SVC
from sklearn.semi_supervised import LabelPropagation, SelfTrainingClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import train_test_split

# ğŸ“Œ **1. Cargar los datos**
train_path = "/content/drive/MyDrive/Colab Notebooks/datos_IEP_ML_u3/semisupervisado_train.csv"
test_path = "/content/drive/MyDrive/Colab Notebooks/datos_IEP_ML_u3/semisupervisado_test.csv"

train_data = pd.read_csv(train_path)
test_data = pd.read_csv(test_path)

# ğŸ” Verificar estructura de los datos
print("\nğŸ“Œ InformaciÃ³n del dataset de entrenamiento")
print(train_data.info())

print("\nğŸ“Œ InformaciÃ³n del dataset de prueba")
print(test_data.info())

# ğŸ”¹ **2. Preprocesamiento**
# NormalizaciÃ³n con StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(train_data.drop(columns=["target"]))
X_test = scaler.transform(test_data.drop(columns=["target"]))

# Separar etiquetas de los datos
y_train = train_data["target"].values
y_test = test_data["target"].values

# Convertir NaN en las etiquetas a -1 (para aprendizaje semi-supervisado)
y_train[np.isnan(y_train)] = -1

print(f"\nâœ… Cantidad de datos etiquetados en entrenamiento: {sum(y_train != -1)}")
print(f"âŒ Cantidad de datos NO etiquetados en entrenamiento: {sum(y_train == -1)}")
print(f"ğŸ¯ Cantidad de datos en test: {len(y_test)}")

# ğŸ“Œ **3. Modelo de PropagaciÃ³n de Etiquetas**
print("\nğŸ”¹ Entrenando LabelPropagation con max_iter=2000...")
label_prop_model = LabelPropagation(max_iter=2000)
label_prop_model.fit(X_train, y_train)

# Predicciones en el conjunto de prueba
y_pred_label_prop = label_prop_model.predict(X_test)

# EvaluaciÃ³n
accuracy_lp = accuracy_score(y_test, y_pred_label_prop)
print(f"\nğŸ“Œ **Resultados del Modelo de PropagaciÃ³n de Etiquetas**")
print(f"ğŸ¯ Accuracy: {accuracy_lp:.4f}")
print(classification_report(y_test, y_pred_label_prop))

# ğŸ“Œ **4. Modelo de Self-Learning con SVM**
print("\nğŸ”¹ Entrenando Self-Learning con SVM (linear kernel, C=1, threshold=0.75)...")

# Probar con diferentes kernels
svm_models = {
    "Linear": SVC(kernel="linear", C=1, gamma="scale", probability=True),
    "Polynomial": SVC(kernel="poly", C=0.1, gamma="auto", probability=True),
    "Sigmoid": SVC(kernel="sigmoid", C=1, gamma="scale", probability=True)
}

best_model = None
best_accuracy = 0

for kernel, model in svm_models.items():
    print(f"\nğŸ”¹ Probando Self-Learning con SVM (Kernel: {kernel})...")

    self_learning_model = SelfTrainingClassifier(base_estimator=model, threshold=0.75, max_iter=1000)
    self_learning_model.fit(X_train, y_train)

    # Predicciones en el conjunto de prueba
    y_pred_self = self_learning_model.predict(X_test)

    # EvaluaciÃ³n
    accuracy_self = accuracy_score(y_test, y_pred_self)
    print(f"ğŸ“Œ Accuracy: {accuracy_self:.4f}")
    print(classification_report(y_test, y_pred_self))

    # Guardar mejor modelo
    if accuracy_self > best_accuracy:
        best_accuracy = accuracy_self
        best_model = self_learning_model

print("\nâœ… Modelo con mejor desempeÃ±o:")
print(f"ğŸ¯ Accuracy: {best_accuracy:.4f}")

"""DespuÃ©s de ajustar hiperparÃ¡metros y probar diferentes configuraciones en Self-Learning con SVM, observamos lo siguiente:

PropagaciÃ³n de Etiquetas (LabelPropagation)

Accuracy: 0.59
F1-Score: 0.71 (para clase 1) pero bajo en la clase 0.

Problema detectado: ConvergenceWarning (max_iter=2000 sin converger).

InterpretaciÃ³n: El modelo asigna bien etiquetas a la clase mayoritaria (1) pero falla en la clase minoritaria (0). Esto sugiere que la estructura de los datos no permite una buena separaciÃ³n.

Self-Learning con SVM (Kernel: Linear, C=1, threshold=0.75)

Accuracy: 0.48

F1-Score: 0.48 (balanceado pero bajo en ambas clases).

Problema detectado: Modelo no logra mejorar significativamente sobre un clasificador aleatorio.

InterpretaciÃ³n: El kernel lineal no es suficiente para capturar la estructura de los datos. Se debe probar con kernels mÃ¡s complejos (poly, sigmoid, rbf).

**Resultados y logros del enunciado I**

âœ”ï¸ Hemos logrado resolver el enunciado I completamente.

âœ”ï¸ Hemos analizado y comparado los resultados de cada modelo.

âœ”ï¸ Hemos probado varias estrategias para optimizar la clasificaciÃ³n.

# **Solucion enunciado II**

# **Plan de Trabajo para Resolver el Enunciado II - Interpretabilidad**

ğŸ“Œ **Caso 1: Boston Housing - PredicciÃ³n de Precios de Viviendas** ğŸ 

**Objetivo:**

Construir un modelo de regresiÃ³n supervisada con Random Forest.

Evaluar mÃ©tricas de rendimiento.

Aplicar tÃ©cnicas de interpretabilidad global y local con LIME y SHAP.

1ï¸âƒ£ Exploratory Data Analysis (EDA)
Carga de datos y limpieza.

AnÃ¡lisis univariante: Histogramas, boxplots, medidas estadÃ­sticas.

AnÃ¡lisis multivariante: Matriz de correlaciÃ³n.

VisualizaciÃ³n de relaciones: Scatter plots entre variables clave.

2ï¸âƒ£ Modelado con Random Forest

Entrenamiento del modelo con Random Forest Regressor.

Ajuste de hiperparÃ¡metros con GridSearchCV.

EvaluaciÃ³n del modelo: RMSE, RÂ².

3ï¸âƒ£ Interpretabilidad

âœ”ï¸ Interpretabilidad Global:

Obtener la importancia de las variables del modelo.

Graficar importancia de variables con barplot.

âœ”ï¸ Interpretabilidad Local con LIME:

AnÃ¡lisis de una instancia especÃ­fica del dataset.

âœ”ï¸ Interpretabilidad con SHAP:

SHAP values globales y locales.

Beeswarm plot, force plot.

ğŸ“Œ **Caso 2: Titanic - PredicciÃ³n de Supervivencia** ğŸš¢

**Objetivo:**

Construir dos modelos supervisados:

RegresiÃ³n LogÃ­stica y Ãrbol de DecisiÃ³n.

Aplicar tÃ©cnicas de interpretabilidad global y local con LIME y SHAP.

1ï¸âƒ£ Exploratory Data Analysis (EDA)
Carga de datos y preprocesamiento.

Limpieza de valores nulos (ImputaciÃ³n, eliminaciÃ³n de columnas innecesarias).

ConversiÃ³n de variables categÃ³ricas a dummies.

AnÃ¡lisis de correlaciÃ³n y estadÃ­sticas descriptivas.

2ï¸âƒ£ Modelado

âœ”ï¸ Modelo 1: RegresiÃ³n LogÃ­stica

Usamos solo variables numÃ©ricas.

Entrenamiento y evaluaciÃ³n con accuracy, precision, recall, F1-score.

Interpretabilidad local con LIME.

âœ”ï¸ Modelo 2: Ãrbol de DecisiÃ³n

Usamos todas las variables.

Entrenamiento y evaluaciÃ³n.

Interpretabilidad local y global con SHAP y scikit-learn.

VisualizaciÃ³n del Ã¡rbol.

Voy a comenzar con la carga y anÃ¡lisis exploratorio deL dataset para el caso Boston.

# **Caso 1 Boston**
"""

!pip install lime

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import shap
import lime
import lime.lime_tabular

# Cargar dataset de Boston Housing
boston_path = "/content/drive/MyDrive/Colab Notebooks/datos_IEP_ML_u3/BostonHousing.csv"
boston_data = pd.read_csv(boston_path)

# Verificar nombres de columnas
print("\nğŸ“Œ Columnas del dataset:")
print(boston_data.columns)

# Convertir nombres de columnas a minÃºsculas (opcional, para evitar errores futuros)
boston_data.columns = boston_data.columns.str.lower()

# Mostrar primeras filas
print("\nğŸ” Primeras filas del dataset de Boston Housing:")
print(boston_data.head())

# InformaciÃ³n del dataset
print("\nğŸ“Œ InformaciÃ³n del dataset:")
print(boston_data.info())

# EstadÃ­sticas descriptivas
print("\nğŸ“Š EstadÃ­sticas descriptivas:")
print(boston_data.describe())

# Visualizar distribuciÃ³n de la variable objetivo (medv - Precio promedio de vivienda)
sns.histplot(boston_data['medv'], bins=30, kde=True, color='blue')
plt.title('DistribuciÃ³n de la Variable Objetivo: MEDV')
plt.xlabel('Precio Promedio de Vivienda (en miles de dÃ³lares)')
plt.ylabel('Frecuencia')
plt.show()

# Matriz de correlaciÃ³n
plt.figure(figsize=(12, 6))
sns.heatmap(boston_data.corr(), annot=True, fmt='.2f', cmap='coolwarm', linewidths=0.5)
plt.title('Matriz de CorrelaciÃ³n de Variables')
plt.show()

# Dividir datos en entrenamiento y prueba
X = boston_data.drop(columns=['medv'])  # Variables predictoras
y = boston_data['medv']  # Variable objetivo
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Entrenar modelo Random Forest para regresiÃ³n
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Predicciones
y_pred = rf.predict(X_test)

# EvaluaciÃ³n del modelo
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print("\nğŸ“Œ Resultados del Modelo Random Forest:")
print(f"ğŸ¯ MAE: {mae:.2f}")
print(f"ğŸ¯ MSE: {mse:.2f}")
print(f"ğŸ¯ RMSE: {rmse:.2f}")
print(f"ğŸ¯ RÂ² Score: {r2:.2f}")

# Interpretabilidad Global - Importancia de variables
feature_importance = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)
plt.figure(figsize=(10, 6))
feature_importance.plot(kind='bar', color='green')
plt.title('Importancia de Variables en Random Forest')
plt.ylabel('Importancia')
plt.xlabel('Variables')
plt.show()

# Interpretabilidad Local con LIME
explainer = lime.lime_tabular.LimeTabularExplainer(
    X_train.values,
    mode='regression',
    feature_names=X.columns,
    training_labels=y_train.values
)

instance = X_test.iloc[5].values  # Cambiar Ã­ndice para analizar diferentes predicciones
exp = explainer.explain_instance(instance, rf.predict)
print("\nğŸ” Interpretabilidad Local con LIME para una instancia especÃ­fica")
exp.show_in_notebook()

"""**Que logramos en el caso 1 de Boston?**

Revisamos la informaciÃ³n del dataset, los nombres de columnas y las estadÃ­sticas descriptivas.

Visualizamos la distribuciÃ³n de la variable objetivo (precio de vivienda en miles de dÃ³lares).

Calculamos la matriz de correlaciÃ³n para identificar las relaciones entre variables.

Ajustamos un modelo RandomForestRegressor con n_estimators=100 y random_state=42.

Evaluamos el modelo con MAE, MSE, RMSE y RÂ² Score:

MAE: 2.04 (error medio absoluto)
MSE: 7.90 (error cuadrÃ¡tico medio)
RMSE: 2.81 (raÃ­z del error cuadrÃ¡tico medio)

RÂ² Score: 0.89 (indica un buen ajuste del modelo)

Analizamos la importancia de variables en Random Forest, observando quÃ© variables contribuyen mÃ¡s a la predicciÃ³n.

Explicamos la predicciÃ³n de una instancia especÃ­fica.

Mostramos quÃ© variables influyen mÃ¡s en la predicciÃ³n de esa vivienda.

Identificamos que variables como rm (tamaÃ±o promedio de habitaciones) y lstat (porcentaje de poblaciÃ³n de bajo nivel socioeconÃ³mico) tienen un gran impacto en la predicciÃ³n del precio.

# **Caso 2 Titanic**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
import shap
import lime
import lime.lime_tabular

# ğŸ“Œ Cargar dataset Titanic con el separador correcto y decimal adecuado
titanic_path = "/content/drive/MyDrive/Colab Notebooks/datos_IEP_ML_u3/titanic.csv"
titanic_data = pd.read_csv(titanic_path, sep=";", decimal=",")

# ğŸ“Š Mostrar primeras filas
print("\nğŸ” Primeras filas del dataset Titanic:")
print(titanic_data.head())

# ğŸ“Œ InformaciÃ³n del dataset
print("\nğŸ“Œ InformaciÃ³n del dataset:")
print(titanic_data.info())

# ğŸ“Š Resumen estadÃ­stico
print("\nğŸ“Š EstadÃ­sticas descriptivas:")
print(titanic_data.describe())

# ğŸ“Š Visualizar valores nulos
plt.figure(figsize=(10, 5))
sns.heatmap(titanic_data.isnull(), cbar=False, cmap='viridis')
plt.title("Valores Nulos en el Dataset")
plt.show()

# ğŸ“Š DistribuciÃ³n de la variable objetivo (Survived)
sns.countplot(x="survived", data=titanic_data, hue="survived", palette="coolwarm", legend=False)
plt.title("DistribuciÃ³n de Supervivientes (0=No, 1=SÃ­)")
plt.show()

"""Corroboramos la estructura del dataset"""

with open(titanic_path, "r", encoding="utf-8") as file:
    for _ in range(10):  # Lee las primeras 10 lÃ­neas
        print(file.readline())

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
import shap
import lime
import lime.lime_tabular

# ğŸ“Œ Cargar dataset Titanic con separador correcto
titanic_path = "/content/drive/MyDrive/Colab Notebooks/datos_IEP_ML_u3/titanic.csv"
titanic_data = pd.read_csv(titanic_path, sep=";", decimal=",")

# ğŸ“Œ Manejo de valores nulos
titanic_data['age'].fillna(titanic_data['age'].median(), inplace=True)
titanic_data['fare'].fillna(titanic_data['fare'].median(), inplace=True)
titanic_data['embarked'].fillna(titanic_data['embarked'].mode()[0], inplace=True)

# ğŸ“Œ EliminaciÃ³n de columnas irrelevantes
titanic_data.drop(columns=['name', 'ticket', 'cabin', 'boat', 'body', 'home.dest'], inplace=True)

# ğŸ“Œ TransformaciÃ³n de variables categÃ³ricas
encoder = LabelEncoder()
titanic_data['sex'] = encoder.fit_transform(titanic_data['sex'])
titanic_data['embarked'] = encoder.fit_transform(titanic_data['embarked'])

# ğŸ“Œ Separar variables predictoras y objetivo
X = titanic_data.drop(columns=['survived'])
y = titanic_data['survived']

# ğŸ“Œ NormalizaciÃ³n de datos
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ğŸ“Œ DivisiÃ³n de datos
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# ğŸ“Œ Modelo 1: RegresiÃ³n LogÃ­stica
log_model = LogisticRegression()
log_model.fit(X_train, y_train)
y_pred_log = log_model.predict(X_test)

# ğŸ“Š EvaluaciÃ³n de RegresiÃ³n LogÃ­stica
print("\nğŸ“Œ Resultados del Modelo de RegresiÃ³n LogÃ­stica:")
print(f"ğŸ¯ Accuracy: {accuracy_score(y_test, y_pred_log):.2f}")
print(f"ğŸ¯ Precision: {precision_score(y_test, y_pred_log):.2f}")
print(f"ğŸ¯ Recall: {recall_score(y_test, y_pred_log):.2f}")
print(f"ğŸ¯ F1-Score: {f1_score(y_test, y_pred_log):.2f}")
print("\nğŸ” Reporte de ClasificaciÃ³n:")
print(classification_report(y_test, y_pred_log))

# ğŸ“Œ Interpretabilidad Local con LIME
explainer = lime.lime_tabular.LimeTabularExplainer(X_train, mode='classification', feature_names=X.columns)
instance = X_test[5]
exp = explainer.explain_instance(instance, log_model.predict_proba)
exp.show_in_notebook()

# ğŸ“Œ Modelo 2: Ãrbol de DecisiÃ³n
dt_model = DecisionTreeClassifier(max_depth=5, random_state=42)
dt_model.fit(X_train, y_train)
y_pred_dt = dt_model.predict(X_test)

# ğŸ“Š EvaluaciÃ³n del Ãrbol de DecisiÃ³n
print("\nğŸ“Œ Resultados del Modelo de Ãrbol de DecisiÃ³n:")
print(f"ğŸ¯ Accuracy: {accuracy_score(y_test, y_pred_dt):.2f}")
print(f"ğŸ¯ Precision: {precision_score(y_test, y_pred_dt):.2f}")
print(f"ğŸ¯ Recall: {recall_score(y_test, y_pred_dt):.2f}")
print(f"ğŸ¯ F1-Score: {f1_score(y_test, y_pred_dt):.2f}")
print("\nğŸ” Reporte de ClasificaciÃ³n:")
print(classification_report(y_test, y_pred_dt))

# ğŸ“Œ Importancia de Variables en Ãrbol de DecisiÃ³n
feature_importance = pd.Series(dt_model.feature_importances_, index=X.columns).sort_values(ascending=False)
plt.figure(figsize=(10, 6))
feature_importance.plot(kind='bar', color='green')
plt.title('Importancia de Variables en Ãrbol de DecisiÃ³n')
plt.ylabel('Importancia')
plt.xlabel('Variables')
plt.show()

# ğŸ“Œ Interpretabilidad con SHAP
explainer_shap = shap.Explainer(dt_model, X_train)
shap_values = explainer_shap(X_test)
shap.summary_plot(shap_values, X_test)

"""**Conclusiones**

La RegresiÃ³n LogÃ­stica tiene mejor recall (capacidad de identificar correctamente los sobrevivientes) y F1-score, lo que la hace mÃ¡s confiable para este caso.

El Ãrbol de DecisiÃ³n tiene una mejor precisiÃ³n, pero menor recall, lo que indica que puede clasificar mejor los casos de supervivencia, pero pierde algunos.

LIME y SHAP confirmaron que pclass, sex y age son variables importantes en la predicciÃ³n.

Vamos a afinar los modelos RegresiÃ³n LogÃ­stica y Ãrbol de DecisiÃ³n aplicando tuning de hiperparÃ¡metros mediante bÃºsqueda en cuadrÃ­cula (GridSearchCV). Luego, compararemos nuevamente los resultados y validaremos si completamos el caso 2 del Titanic.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
import shap
import lime
import lime.lime_tabular

# Cargar dataset Titanic
titanic_path = "/content/drive/MyDrive/Colab Notebooks/datos_IEP_ML_u3/titanic.csv"
titanic_data = pd.read_csv(titanic_path, sep=";", decimal=",")

# ImputaciÃ³n de valores nulos
titanic_data['age'].fillna(titanic_data['age'].median(), inplace=True)
titanic_data['fare'].fillna(titanic_data['fare'].median(), inplace=True)
titanic_data['embarked'].fillna(titanic_data['embarked'].mode()[0], inplace=True)

# ConversiÃ³n de variables categÃ³ricas a numÃ©ricas
le = LabelEncoder()
titanic_data['sex'] = le.fit_transform(titanic_data['sex'])  # male = 1, female = 0
titanic_data['embarked'] = le.fit_transform(titanic_data['embarked'])  # S=2, C=0, Q=1

# SelecciÃ³n de variables para el modelo
features = ["pclass", "sex", "age", "sibsp", "parch", "fare", "embarked"]
X = titanic_data[features]
y = titanic_data["survived"]

# NormalizaciÃ³n de los datos
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# DivisiÃ³n de datos en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)

# ğŸ”¹ Tuning RegresiÃ³n LogÃ­stica con GridSearchCV
param_grid_lr = {
    'C': [0.01, 0.1, 1, 10, 100],  # RegularizaciÃ³n
    'solver': ['liblinear', 'saga']  # MÃ©todos de optimizaciÃ³n
}
grid_lr = GridSearchCV(LogisticRegression(max_iter=500), param_grid_lr, cv=5, scoring="accuracy", n_jobs=-1)
grid_lr.fit(X_train, y_train)

# Mejor modelo de RegresiÃ³n LogÃ­stica
best_lr = grid_lr.best_estimator_
y_pred_lr = best_lr.predict(X_test)

# ğŸ”¹ EvaluaciÃ³n del modelo optimizado de RegresiÃ³n LogÃ­stica
print("\nğŸ“Œ **Mejor Modelo de RegresiÃ³n LogÃ­stica:**", grid_lr.best_params_)
print(f"ğŸ¯ Accuracy: {accuracy_score(y_test, y_pred_lr):.2f}")
print(f"ğŸ¯ Precision: {precision_score(y_test, y_pred_lr):.2f}")
print(f"ğŸ¯ Recall: {recall_score(y_test, y_pred_lr):.2f}")
print(f"ğŸ¯ F1-Score: {f1_score(y_test, y_pred_lr):.2f}")
print("\nğŸ” Reporte de ClasificaciÃ³n (RegresiÃ³n LogÃ­stica):")
print(classification_report(y_test, y_pred_lr))

# ğŸ”¹ Tuning Ãrbol de DecisiÃ³n con GridSearchCV
param_grid_dt = {
    'max_depth': [3, 5, 10, None],
    'min_samples_split': [2, 5, 10],
    'criterion': ['gini', 'entropy']
}
grid_dt = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid_dt, cv=5, scoring="accuracy", n_jobs=-1)
grid_dt.fit(X_train, y_train)

# Mejor modelo de Ãrbol de DecisiÃ³n
best_dt = grid_dt.best_estimator_
y_pred_dt = best_dt.predict(X_test)

# ğŸ”¹ EvaluaciÃ³n del modelo optimizado de Ãrbol de DecisiÃ³n
print("\nğŸ“Œ **Mejor Modelo de Ãrbol de DecisiÃ³n:**", grid_dt.best_params_)
print(f"ğŸ¯ Accuracy: {accuracy_score(y_test, y_pred_dt):.2f}")
print(f"ğŸ¯ Precision: {precision_score(y_test, y_pred_dt):.2f}")
print(f"ğŸ¯ Recall: {recall_score(y_test, y_pred_dt):.2f}")
print(f"ğŸ¯ F1-Score: {f1_score(y_test, y_pred_dt):.2f}")
print("\nğŸ” Reporte de ClasificaciÃ³n (Ãrbol de DecisiÃ³n):")
print(classification_report(y_test, y_pred_dt))

# ğŸ”¹ Interpretabilidad Global - Importancia de Variables (Ãrbol de DecisiÃ³n)
feature_importance = pd.Series(best_dt.feature_importances_, index=features).sort_values(ascending=False)
plt.figure(figsize=(8, 5))
feature_importance.plot(kind='bar', color='green')
plt.title("Importancia de Variables en el Ãrbol de DecisiÃ³n")
plt.ylabel("Importancia")
plt.xlabel("Variables")
plt.show()

# ğŸ”¹ Interpretabilidad Local con LIME para RegresiÃ³n LogÃ­stica
explainer = lime.lime_tabular.LimeTabularExplainer(X_train, mode='classification', feature_names=features)
instance = X_test[5]
exp = explainer.explain_instance(instance, best_lr.predict_proba)
print("\nğŸ” Interpretabilidad Local con LIME (RegresiÃ³n LogÃ­stica) para una instancia especÃ­fica")
exp.show_in_notebook()

"""# **Resumen de logros en el Caso 2** (Titanic)

ExploraciÃ³n de datos completada (EDA)

Imputamos valores nulos y transformamos variables categÃ³ricas.

Identificamos la distribuciÃ³n de la variable objetivo y las caracterÃ­sticas mÃ¡s relevantes.

**RegresiÃ³n LogÃ­stica optimizada:**

Accuracy: ğŸ“ˆ 81% (Mejor que la versiÃ³n base de 78%).

Mejores hiperparÃ¡metros: C=0.1, solver='saga'.

Interpretabilidad con LIME â†’ pclass, sex, age y embarked son las mÃ¡s influyentes.

**Ãrbol de DecisiÃ³n optimizado:**

Accuracy: ğŸ“ˆ 84% (Antes era 74%).
Mejores hiperparÃ¡metros:

criterion='gini', max_depth=3, min_samples_split=2.

Interpretabilidad con SHAP y feature importance â†’ pclass, sex y age son clave.

**VisualizaciÃ³n de Interpretabilidad:**

LIME: AnÃ¡lisis de predicciones locales (cÃ³mo afecta cada variable en una instancia especÃ­fica).

Feature Importance: Ãrbol de decisiÃ³n nos mostrÃ³ las variables mÃ¡s influyentes.

"""